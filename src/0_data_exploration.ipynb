{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from ebrec.utils._constants import * # a bunch of constant strings for column names\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "data_base = Path(os.getcwd()).parent / \"data-merged\" / \"merged\"\n",
    "train_val_base = data_base / \"1-ebnerd_demo_(20MB)\"\n",
    "# train_val_base = data_base / \"3-ebnerd_large_(3.0GB)\"\n",
    "test_base = data_base / \"5-ebnerd_testset_(1.5GB)\"\n",
    "assert train_val_base.exists() and test_base.exists()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "load user history\n",
    "\"\"\"\n",
    "\n",
    "train_behaviors = pl.scan_parquet(train_val_base / \"train\" / \"behaviors.parquet\")\n",
    "train_history = pl.scan_parquet(train_val_base / \"train\" / \"history.parquet\")\n",
    "\n",
    "val_behavior = pl.scan_parquet(train_val_base / \"validation\" / \"behaviors.parquet\")\n",
    "val_history = pl.scan_parquet(train_val_base / \"validation\" / \"history.parquet\")\n",
    "\n",
    "test_behavior = pl.scan_parquet(test_base / \"test\" / \"behaviors.parquet\")\n",
    "test_history = pl.scan_parquet(test_base / \"test\" / \"history.parquet\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "merge history and behavior\n",
    "\"\"\"\n",
    "\n",
    "assert list(set(train_behaviors.columns).intersection(set(train_history.columns)))[0] == DEFAULT_USER_COL\n",
    "train_user: pl.LazyFrame = train_behaviors.join(train_history, on=DEFAULT_USER_COL, how=\"inner\")\n",
    "\n",
    "assert list(set(val_behavior.columns).intersection(set(val_history.columns)))[0] == DEFAULT_USER_COL\n",
    "val_user: pl.LazyFrame = val_behavior.join(val_history, on=DEFAULT_USER_COL, how=\"inner\")\n",
    "\n",
    "assert list(set(test_behavior.columns).intersection(set(test_history.columns)))[0] == DEFAULT_USER_COL\n",
    "test_user: pl.LazyFrame = test_behavior.join(test_history, on=DEFAULT_USER_COL, how=\"inner\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "load article content\n",
    "\"\"\"\n",
    "\n",
    "train_articles: pl.LazyFrame = pl.scan_parquet(train_val_base / \"articles.parquet\")\n",
    "val_articles: pl.LazyFrame = train_articles\n",
    "test_articles: pl.LazyFrame = pl.scan_parquet(test_base / \"articles.parquet\")\n",
    "\n",
    "articles_word2vec: pl.LazyFrame = pl.scan_parquet(data_base / \"7-Ekstra-Bladet-word2vec_(133MB)\" / \"document_vector.parquet\")\n",
    "articles_image_embeddings: pl.LazyFrame = pl.scan_parquet(data_base / \"8-Ekstra_Bladet_image_embeddings_(372MB)\" / \"image_embeddings.parquet\")\n",
    "articles_contrastive_vector: pl.LazyFrame = pl.scan_parquet(data_base / \"9-Ekstra-Bladet-contrastive_vector_(341MB)\" / \"contrastive_vector.parquet\")\n",
    "articles_bert_base_multilingual_cased: pl.LazyFrame = pl.scan_parquet(data_base / \"10-google-bert-base-multilingual-cased_(344MB)\" / \"bert_base_multilingual_cased.parquet\")\n",
    "articles_xlm_roberta_base: pl.LazyFrame = pl.scan_parquet(data_base / \"11-FacebookAI-xlm-roberta-base_(341MB)\" / \"xlm_roberta_base.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check sparsity of cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the dataset is massive.\n",
    "\n",
    "ever single column that we can drop will save us a lot of time and memory.\n",
    "\n",
    "user data:\n",
    "\n",
    "> drop `gender`, `postcode`, `age`, `scroll_percentage`\n",
    "\n",
    "- the null value in the `article_id` means that the impression was left from the front page, so it is not a missing value.\n",
    "- all user sets (train/val/test) have too few `gender`, `postcode`, `age`, `scroll_percentage` values for these columns to be useful.\n",
    "\n",
    "articles data:\n",
    "\n",
    "> don't drop anything\n",
    "\n",
    "- the `total_pageviews`, `total_inviews`, `total_read_time` columns reflect the popularity of the article in the first 7 days after publication. they aren't missing values.\n",
    "- since only 88.59% of articles have `image_ids`. fewer images also mean fewer image embeddings (reduces completeness, not predictive power)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user train\n",
      "\t- article_id: 29.89% CRITICAL\n",
      "\t- scroll_percentage: 29.34% CRITICAL\n",
      "\t- gender: 7.35% CRITICAL\n",
      "\t- postcode: 2.23% CRITICAL\n",
      "\t- age: 3.06% CRITICAL\n",
      "\t- next_read_time: 97.35% \n",
      "\t- next_scroll_percentage: 88.72% \n"
     ]
    }
   ],
   "source": [
    "print(\"user train\")\n",
    "total_train: int = train_user.select(DEFAULT_USER_COL).count().collect().to_dicts()[0][DEFAULT_USER_COL]\n",
    "for col in train_user.columns:\n",
    "    non_null: int = train_user.select(col).drop_nulls().count().collect().to_dicts()[0][col]\n",
    "    non_null_ratio = non_null / total_train\n",
    "    if non_null_ratio < 1.0:\n",
    "        print(f\"\\t- {col}: {non_null_ratio * 100:.2f}% {'CRITICAL' if non_null_ratio < 0.5 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user val\n",
      "\t- article_id: 29.02% CRITICAL\n",
      "\t- scroll_percentage: 28.65% CRITICAL\n",
      "\t- gender: 7.07% CRITICAL\n",
      "\t- postcode: 2.15% CRITICAL\n",
      "\t- age: 2.94% CRITICAL\n",
      "\t- next_read_time: 97.29% \n",
      "\t- next_scroll_percentage: 88.05% \n"
     ]
    }
   ],
   "source": [
    "print(\"user val\")\n",
    "total_val: int = val_user.select(DEFAULT_USER_COL).count().collect().to_dicts()[0][DEFAULT_USER_COL]\n",
    "for col in val_user.columns:\n",
    "    non_null: int = val_user.select(col).drop_nulls().count().collect().to_dicts()[0][col]\n",
    "    non_null_ratio = non_null / total_val\n",
    "    if non_null_ratio < 1.0:\n",
    "        print(f\"\\t- {col}: {non_null_ratio * 100:.2f}% {'CRITICAL' if non_null_ratio < 0.5 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user test\n",
      "\t- scroll_percentage: 28.44% CRITICAL\n",
      "\t- gender: 7.07% CRITICAL\n",
      "\t- postcode: 2.15% CRITICAL\n",
      "\t- age: 2.94% CRITICAL\n"
     ]
    }
   ],
   "source": [
    "print(\"user test\")\n",
    "total_test: int = test_user.select(DEFAULT_USER_COL).count().collect().to_dicts()[0][DEFAULT_USER_COL]\n",
    "for col in test_user.columns:\n",
    "    non_null: int = test_user.select(col).drop_nulls().count().collect().to_dicts()[0][col]\n",
    "    non_null_ratio = non_null / total_test\n",
    "    if non_null_ratio < 1.0:\n",
    "        print(f\"\\t- {col}: {non_null_ratio * 100:.2f}% {'CRITICAL' if non_null_ratio < 0.5 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/val articles\n",
      "\t- image_ids: 88.59% \n",
      "\t- total_inviews: 14.61% CRITICAL\n",
      "\t- total_pageviews: 13.49% CRITICAL\n",
      "\t- total_read_time: 13.49% CRITICAL\n"
     ]
    }
   ],
   "source": [
    "print(\"train/val articles\")\n",
    "total_articles: int = train_articles.select(DEFAULT_ARTICLE_ID_COL).count().collect().to_dicts()[0][DEFAULT_ARTICLE_ID_COL]\n",
    "for col in train_articles.columns:\n",
    "    non_null: int = train_articles.select(col).drop_nulls().count().collect().to_dicts()[0][col]\n",
    "    non_null_ratio = non_null / total_articles\n",
    "    if non_null_ratio < 1.0:\n",
    "        print(f\"\\t- {col}: {non_null_ratio * 100:.2f}% {'CRITICAL' if non_null_ratio < 0.5 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test articles\n",
      "\t- image_ids: 88.59% \n",
      "\t- total_inviews: 14.61% CRITICAL\n",
      "\t- total_pageviews: 13.49% CRITICAL\n",
      "\t- total_read_time: 13.49% CRITICAL\n"
     ]
    }
   ],
   "source": [
    "print(\"test articles\")\n",
    "total_articles: int = test_articles.select(DEFAULT_ARTICLE_ID_COL).count().collect().to_dicts()[0][DEFAULT_ARTICLE_ID_COL]\n",
    "for col in test_articles.columns:\n",
    "    non_null: int = test_articles.select(col).drop_nulls().count().collect().to_dicts()[0][col]\n",
    "    non_null_ratio = non_null / total_articles\n",
    "    if non_null_ratio < 1.0:\n",
    "        print(f\"\\t- {col}: {non_null_ratio * 100:.2f}% {'CRITICAL' if non_null_ratio < 0.5 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec [{'article_id': 125541, 'document_vector': 125541}]\n",
      "image embeddings [{'article_id': 102603, 'image_embedding': 102603}]\n",
      "contrastive vector [{'article_id': 125541, 'contrastive_vector': 125541}]\n",
      "bert base multilingual cased [{'article_id': 125541, 'google-bert/bert-base-multilingual-cased': 125541}]\n",
      "xlm roberta base [{'article_id': 125541, 'FacebookAI/xlm-roberta-base': 125541}]\n"
     ]
    }
   ],
   "source": [
    "print(\"word2vec\", articles_word2vec.count().collect().to_dicts())\n",
    "print(\"image embeddings\", articles_image_embeddings.count().collect().to_dicts())\n",
    "print(\"contrastive vector\", articles_contrastive_vector.count().collect().to_dicts())\n",
    "print(\"bert base multilingual cased\", articles_bert_base_multilingual_cased.count().collect().to_dicts())\n",
    "print(\"xlm roberta base\", articles_xlm_roberta_base.count().collect().to_dicts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
