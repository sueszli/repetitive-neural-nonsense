{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTUR: Neural News Recommendation with Long- and Short-term User Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sueszli/.asdf/installs/python/3.11.9/lib/python3.11/site-packages/ebrec/utils/_behaviors.py:619: UserWarning: truncate_history: The history IDs expeced in ascending order\n",
      "  warnings.warn(f\"{function_name}: The history IDs expeced in ascending order\")\n",
      "/Users/sueszli/.asdf/installs/python/3.11.9/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from ebrec.utils._constants import *\n",
    "from ebrec.utils._behaviors import create_binary_labels_column, sampling_strategy_wu2019, add_known_user_column, add_prediction_scores, truncate_history\n",
    "from ebrec.utils._articles import convert_text2encoding_with_transformers, create_article_id_to_value_mapping\n",
    "from ebrec.utils._polars import concat_str_columns, slice_join_dataframes\n",
    "from ebrec.utils._nlp import get_transformers_word_embeddings\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "load data\n",
    "\"\"\"\n",
    "\n",
    "data_base = Path(os.getcwd()).parent / \"data-merged\" / \"merged\"\n",
    "# train_val_base = data_base / \"1-ebnerd_demo_(20MB)\"\n",
    "train_val_base = data_base / \"2-ebnerd_small_(80MB)\"\n",
    "# train_val_base = data_base / \"3-ebnerd_large_(3.0GB)\"\n",
    "test_base = data_base / \"5-ebnerd_testset_(1.5GB)\"\n",
    "assert train_val_base.exists() and test_base.exists()\n",
    "\n",
    "train_behaviors = pl.scan_parquet(train_val_base / \"train\" / \"behaviors.parquet\")\n",
    "train_history = pl.scan_parquet(train_val_base / \"train\" / \"history.parquet\")\n",
    "\n",
    "val_behavior = pl.scan_parquet(train_val_base / \"validation\" / \"behaviors.parquet\")\n",
    "val_history = pl.scan_parquet(train_val_base / \"validation\" / \"history.parquet\")\n",
    "\n",
    "test_behavior = pl.scan_parquet(test_base / \"test\" / \"behaviors.parquet\")\n",
    "test_history = pl.scan_parquet(test_base / \"test\" / \"history.parquet\")\n",
    "\n",
    "train_articles: pl.LazyFrame = pl.scan_parquet(train_val_base / \"articles.parquet\")\n",
    "val_articles: pl.LazyFrame = train_articles\n",
    "test_articles: pl.LazyFrame = pl.scan_parquet(test_base / \"articles.parquet\")\n",
    "\n",
    "articles_word2vec: pl.LazyFrame = pl.scan_parquet(data_base / \"7-Ekstra-Bladet-word2vec_(133MB)\" / \"document_vector.parquet\")\n",
    "articles_image_embeddings: pl.LazyFrame = pl.scan_parquet(data_base / \"8-Ekstra_Bladet_image_embeddings_(372MB)\" / \"image_embeddings.parquet\")\n",
    "articles_contrastive_vector: pl.LazyFrame = pl.scan_parquet(data_base / \"9-Ekstra-Bladet-contrastive_vector_(341MB)\" / \"contrastive_vector.parquet\")\n",
    "articles_bert_base_multilingual_cased: pl.LazyFrame = pl.scan_parquet(data_base / \"10-google-bert-base-multilingual-cased_(344MB)\" / \"bert_base_multilingual_cased.parquet\")\n",
    "articles_xlm_roberta_base: pl.LazyFrame = pl.scan_parquet(data_base / \"11-FacebookAI-xlm-roberta-base_(341MB)\" / \"xlm_roberta_base.parquet\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "preprocessing: truncate user history, select subset of columns, join behavior and history, sample based on Wu2019, add binary labels column\n",
    "\"\"\"\n",
    "\n",
    "def ebnerd_from_path(history: pl.LazyFrame, behaviors: pl.LazyFrame, history_size: int = 30) -> pl.DataFrame:\n",
    "    df_history = history \\\n",
    "            .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL) \\\n",
    "            .pipe(truncate_history, column=DEFAULT_HISTORY_ARTICLE_ID_COL, history_size=history_size, padding_value=0)\n",
    "    df_behaviors = behaviors \\\n",
    "            .collect() \\\n",
    "            .pipe(slice_join_dataframes, df2=df_history.collect(), on=DEFAULT_USER_COL, how=\"left\")\n",
    "    return df_behaviors\n",
    "\n",
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "]\n",
    "HISTORY_SIZE = 30\n",
    "N_SAMPLES = 100\n",
    "df_train = ebnerd_from_path(history=train_history, behaviors=train_behaviors, history_size=HISTORY_SIZE) \\\n",
    "            .select(COLUMNS) \\\n",
    "            .pipe(sampling_strategy_wu2019, npratio=4, shuffle=True, with_replacement=True, seed=123) \\\n",
    "            .pipe(create_binary_labels_column) \\\n",
    "            .sample(n=N_SAMPLES)\n",
    "df_validation = ebnerd_from_path(history=val_history, behaviors=val_behavior, history_size=HISTORY_SIZE) \\\n",
    "            .select(COLUMNS) \\\n",
    "            .pipe(create_binary_labels_column) \\\n",
    "            .sample(n=N_SAMPLES)\n",
    "df_test = ebnerd_from_path(history=test_history, behaviors=val_behavior, history_size=HISTORY_SIZE) \\\n",
    "            .with_columns(pl.Series(DEFAULT_CLICKED_ARTICLES_COL, [[]])) \\\n",
    "            .select(COLUMNS) \\\n",
    "            .pipe(create_binary_labels_column) \\\n",
    "            .sample(n=N_SAMPLES)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "use huggingface transformers to convert article text to tokens, tokens to embeddings\n",
    "\"\"\"\n",
    "\n",
    "df_articles = train_articles.collect()\n",
    "TRANSFORMER_MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "TEXT_COLUMNS_TO_USE = [DEFAULT_SUBTITLE_COL, DEFAULT_TITLE_COL]\n",
    "MAX_TITLE_LENGTH = 30\n",
    "\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH)\n",
    "article_mapping = create_article_id_to_value_mapping(df=df_articles, value_col=token_col_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 400), dtype=tf.float32, name=None), name='att_layer2_24/Sum_1:0', description=\"created by layer 'att_layer2_24'\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LSTURDataLoader.__init__() got an unexpected keyword argument 'x_in'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 44\u001b[0m\n\u001b[1;32m     20\u001b[0m NPRATIO \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# his_input_title_shape = (HISTORY_SIZE, TITLE_SIZE) # (50, 30)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# pred_input_title_shape = (NPRATIO + 1, TITLE_SIZE) # (5, 30)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# vocab_size = word_embeddings.shape[0] # 119547\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# self, x_in, y_in, batch_size, shuffle=True\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mLSTURDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# model=model,\u001b[39;49;00m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mDEFAULT_USER_COL\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mDEFAULT_HISTORY_ARTICLE_ID_COL\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mDEFAULT_CLICKED_ARTICLES_COL\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     50\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03mtrain, predict\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# model.model.fit(input, label_data)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# model.model.predict(input)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: LSTURDataLoader.__init__() got an unexpected keyword argument 'x_in'"
     ]
    }
   ],
   "source": [
    "from ebrec.models.newsrec.dataloader import LSTURDataLoader\n",
    "from ebrec.models.newsrec.model_config import hparams_lstur\n",
    "from ebrec.models.newsrec.lstur import LSTURModel\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "define model\n",
    "\"\"\"\n",
    "config = hparams_lstur\n",
    "word_embeddings = word2vec_embedding\n",
    "model = LSTURModel(hparams=config, word2vec_embedding=word_embeddings)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "create random batch data\n",
    "\"\"\"\n",
    "BATCH_SIZE = 300\n",
    "HISTORY_SIZE = config.history_size\n",
    "TITLE_SIZE = config.title_size\n",
    "NPRATIO = 4\n",
    "\n",
    "# his_input_title_shape = (HISTORY_SIZE, TITLE_SIZE) # (50, 30)\n",
    "# pred_input_title_shape = (NPRATIO + 1, TITLE_SIZE) # (5, 30)\n",
    "# vocab_size = word_embeddings.shape[0] # 119547\n",
    "# n_users = config.n_users # 50000\n",
    "# label_shape = (NPRATIO + 1,) # (5,)\n",
    "# user_indexes_shape = (1,) # (1,)\n",
    "\n",
    "# his_input_title = np.random.randint(0, vocab_size, (BATCH_SIZE, *his_input_title_shape))\n",
    "# pred_input_title = np.random.randint(0, vocab_size, (BATCH_SIZE, *pred_input_title_shape))\n",
    "# user_indexes = np.random.randint(0, n_users, size=(BATCH_SIZE, *user_indexes_shape))\n",
    "# label_data = np.zeros((BATCH_SIZE, *label_shape), dtype=int)\n",
    "# for row in label_data:\n",
    "#     row[np.random.choice(label_shape[0])] = 1\n",
    "\n",
    "# print(his_input_title.shape)\n",
    "# print(pred_input_title.shape)\n",
    "# print(user_indexes.shape)\n",
    "# print(label_data.shape)\n",
    "\n",
    "# input = (user_indexes, his_input_title, pred_input_title)\n",
    "\n",
    "train_dataloader = LSTURDataLoader(\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "train, predict\n",
    "\"\"\"\n",
    "# model.model.fit(input, label_data)\n",
    "# model.model.predict(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
