{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-11 00:21:34.176197: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2024-05-11 00:21:34.176225: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-05-11 00:21:34.176234: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-05-11 00:21:34.176270: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-05-11 00:21:34.176284: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 400), dtype=tf.float32, name=None), name='att_layer2/Sum_1:0', description=\"created by layer 'att_layer2'\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, None, 30)]           0         []                            \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)        [(None, 50, 30)]             0         []                            \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDi  (None, None, 400)            300800    ['input_2[0][0]']             \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " user_encoder (Functional)   (None, 400)                  2126360   ['input_1[0][0]',             \n",
      "                                                          0          'input_4[0][0]']             \n",
      "                                                                                                  \n",
      " dot (Dot)                   (None, None)                 0         ['time_distributed_1[0][0]',  \n",
      "                                                                     'user_encoder[0][0]']        \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, None)                 0         ['dot[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21263600 (81.11 MB)\n",
      "Trainable params: 21263600 (81.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "(300, 50, 30)\n",
      "(300, 5, 30)\n",
      "(300, 1)\n",
      "(300, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-11 00:21:36.817586: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-05-11 00:21:36.945177: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/AssignAddVariableOp_14.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 11s 461ms/step - loss: 1.6133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1715379707.016849       1 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"_has_manual_control_dependencies\" value { b: true } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 49ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.20042472, 0.22490296, 0.20391975, 0.1897768 , 0.18097578],\n",
       "       [0.19249606, 0.21868879, 0.2050681 , 0.17383885, 0.20990819],\n",
       "       [0.22947909, 0.21508436, 0.18182603, 0.17216888, 0.20144172],\n",
       "       ...,\n",
       "       [0.1854658 , 0.18727688, 0.22403893, 0.20266993, 0.20054844],\n",
       "       [0.20902286, 0.21204741, 0.19030018, 0.2216475 , 0.16698207],\n",
       "       [0.19700955, 0.1825246 , 0.20179047, 0.20683563, 0.21183978]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ebrec.models.newsrec.model_config import hparams_lstur\n",
    "from ebrec.models.newsrec.lstur import LSTURModel\n",
    "import numpy as np\n",
    "\n",
    "config = hparams_lstur\n",
    "\n",
    "# Define the number of samples in your batch\n",
    "BATCH_SIZE = 300\n",
    "HISTORY_SIZE = config.history_size\n",
    "TITLE_SIZE = config.title_size\n",
    "NPRATIO = 4\n",
    "word_embeddings = np.random.rand(1000, 100)\n",
    "\n",
    "# Define the shapes of the input data\n",
    "his_input_title_shape = (HISTORY_SIZE, TITLE_SIZE)\n",
    "pred_input_title_shape = (NPRATIO + 1, TITLE_SIZE)\n",
    "vocab_size = word_embeddings.shape[0]\n",
    "n_users = config.n_users\n",
    "label_shape = (NPRATIO + 1,)\n",
    "user_indexes_shape = (1,)\n",
    "\n",
    "model = LSTURModel(hparams=config, word2vec_embedding=word_embeddings)\n",
    "model.model.summary()\n",
    "\n",
    "# Generate some random input data for input_1 with values between 0 and 1\n",
    "his_input_title = np.random.randint(0, vocab_size, (BATCH_SIZE, *his_input_title_shape))\n",
    "# Generate some random input data for input_2 with values between 0 and 1\n",
    "pred_input_title = np.random.randint(0, vocab_size, (BATCH_SIZE, *pred_input_title_shape))\n",
    "# Input data for user_indexes\n",
    "user_indexes = np.random.randint(0, n_users, size=(BATCH_SIZE, *user_indexes_shape))\n",
    "\n",
    "# Generate some random label data with values between 0 and 1\n",
    "label_data = np.zeros((BATCH_SIZE, *label_shape), dtype=int)\n",
    "for row in label_data:\n",
    "    row[np.random.choice(label_shape[0])] = 1\n",
    "\n",
    "# Print the shapes of the input data to verify they match the model's input layers\n",
    "print(his_input_title.shape)\n",
    "print(pred_input_title.shape)\n",
    "print(user_indexes.shape)\n",
    "print(label_data.shape)\n",
    "\n",
    "# Make input for model:\n",
    "input = (user_indexes, his_input_title, pred_input_title)\n",
    "\n",
    "# fit/predict:\n",
    "model.model.fit(input, label_data)\n",
    "model.model.predict(input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-11 00:18:36.960394: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2024-05-11 00:18:36.960615: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-05-11 00:18:36.960624: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-05-11 00:18:36.960668: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-05-11 00:18:36.960981: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "2024-05-11 00:18:39.318974: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-05-11 00:18:39.429383: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/AssignAddVariableOp_14.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 1.6096\n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to ../runs/data/state_dict/NRMS/weights\n",
      "2/2 [==============================] - 65s 57s/step - loss: 1.6096 - val_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x411782710>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "import os\n",
    "\n",
    "from ebrec.utils._constants import *\n",
    "from ebrec.utils._behaviors import create_binary_labels_column, sampling_strategy_wu2019, add_known_user_column, add_prediction_scores, truncate_history\n",
    "from ebrec.utils._articles import convert_text2encoding_with_transformers, create_article_id_to_value_mapping\n",
    "from ebrec.utils._polars import concat_str_columns, slice_join_dataframes\n",
    "from ebrec.utils._nlp import get_transformers_word_embeddings\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from ebrec.models.newsrec.dataloader import NRMSDataLoader\n",
    "from ebrec.models.newsrec.model_config import hparams_nrms\n",
    "from ebrec.models.newsrec import NRMSModel\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "load data\n",
    "\"\"\"\n",
    "\n",
    "data_base = Path(os.getcwd()).parent / \"data-merged\" / \"merged\"\n",
    "# train_val_base = data_base / \"1-ebnerd_demo_(20MB)\"\n",
    "train_val_base = data_base / \"2-ebnerd_small_(80MB)\"\n",
    "# train_val_base = data_base / \"3-ebnerd_large_(3.0GB)\"\n",
    "test_base = data_base / \"5-ebnerd_testset_(1.5GB)\"\n",
    "assert train_val_base.exists() and test_base.exists()\n",
    "\n",
    "train_behaviors = pl.scan_parquet(train_val_base / \"train\" / \"behaviors.parquet\")\n",
    "train_history = pl.scan_parquet(train_val_base / \"train\" / \"history.parquet\")\n",
    "\n",
    "val_behavior = pl.scan_parquet(train_val_base / \"validation\" / \"behaviors.parquet\")\n",
    "val_history = pl.scan_parquet(train_val_base / \"validation\" / \"history.parquet\")\n",
    "\n",
    "test_behavior = pl.scan_parquet(test_base / \"test\" / \"behaviors.parquet\")\n",
    "test_history = pl.scan_parquet(test_base / \"test\" / \"history.parquet\")\n",
    "\n",
    "train_articles: pl.LazyFrame = pl.scan_parquet(train_val_base / \"articles.parquet\")\n",
    "val_articles: pl.LazyFrame = train_articles\n",
    "test_articles: pl.LazyFrame = pl.scan_parquet(test_base / \"articles.parquet\")\n",
    "\n",
    "articles_word2vec: pl.LazyFrame = pl.scan_parquet(data_base / \"7-Ekstra-Bladet-word2vec_(133MB)\" / \"document_vector.parquet\")\n",
    "articles_image_embeddings: pl.LazyFrame = pl.scan_parquet(data_base / \"8-Ekstra_Bladet_image_embeddings_(372MB)\" / \"image_embeddings.parquet\")\n",
    "articles_contrastive_vector: pl.LazyFrame = pl.scan_parquet(data_base / \"9-Ekstra-Bladet-contrastive_vector_(341MB)\" / \"contrastive_vector.parquet\")\n",
    "articles_bert_base_multilingual_cased: pl.LazyFrame = pl.scan_parquet(data_base / \"10-google-bert-base-multilingual-cased_(344MB)\" / \"bert_base_multilingual_cased.parquet\")\n",
    "articles_xlm_roberta_base: pl.LazyFrame = pl.scan_parquet(data_base / \"11-FacebookAI-xlm-roberta-base_(341MB)\" / \"xlm_roberta_base.parquet\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "preprocessing: truncate user history, select subset of columns, join behavior and history, sample based on Wu2019, add binary labels column\n",
    "\"\"\"\n",
    "\n",
    "def ebnerd_from_path(history: pl.LazyFrame, behaviors: pl.LazyFrame, history_size: int = 30) -> pl.DataFrame:\n",
    "    df_history = history \\\n",
    "            .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL) \\\n",
    "            .pipe(truncate_history, column=DEFAULT_HISTORY_ARTICLE_ID_COL, history_size=history_size, padding_value=0)\n",
    "    df_behaviors = behaviors.collect() \\\n",
    "            .pipe(slice_join_dataframes, df2=df_history.collect(), on=DEFAULT_USER_COL, how=\"left\")\n",
    "    return df_behaviors\n",
    "\n",
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "]\n",
    "HISTORY_SIZE = 30\n",
    "N_SAMPLES = 100\n",
    "df_train = ebnerd_from_path(history=train_history, behaviors=train_behaviors, history_size=HISTORY_SIZE) \\\n",
    "            .select(COLUMNS) \\\n",
    "            .pipe(sampling_strategy_wu2019, npratio=4, shuffle=True, with_replacement=True, seed=123) \\\n",
    "            .pipe(create_binary_labels_column) \\\n",
    "            .sample(n=N_SAMPLES)\n",
    "df_validation = ebnerd_from_path(history=val_history, behaviors=val_behavior, history_size=HISTORY_SIZE) \\\n",
    "            .select(COLUMNS) \\\n",
    "            .pipe(create_binary_labels_column) \\\n",
    "            .sample(n=N_SAMPLES)\n",
    "df_test = ebnerd_from_path(history=test_history, behaviors=val_behavior, history_size=HISTORY_SIZE) \\\n",
    "            .with_columns(pl.Series(DEFAULT_CLICKED_ARTICLES_COL, [[]])) \\\n",
    "            .select(COLUMNS) \\\n",
    "            .pipe(create_binary_labels_column) \\\n",
    "            .sample(n=N_SAMPLES)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "use huggingface transformers to convert article text to tokens, tokens to embeddings\n",
    "\"\"\"\n",
    "\n",
    "df_articles = train_articles.collect()\n",
    "TRANSFORMER_MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "TEXT_COLUMNS_TO_USE = [DEFAULT_SUBTITLE_COL, DEFAULT_TITLE_COL]\n",
    "MAX_TITLE_LENGTH = 30\n",
    "\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH)\n",
    "article_mapping = create_article_id_to_value_mapping(df=df_articles, value_col=token_col_title)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "batch data\n",
    "\"\"\"\n",
    "\n",
    "train_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=64,\n",
    ")\n",
    "val_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    "    batch_size=32,\n",
    ")\n",
    "test_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_test,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "train model\n",
    "\"\"\"\n",
    "MODEL_NAME = \"NRMS\"\n",
    "LOG_DIR = f\"../runs/{MODEL_NAME}\"\n",
    "MODEL_WEIGHTS = f\"../runs/data/state_dict/{MODEL_NAME}/weights\"\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=1)\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2)\n",
    "modelcheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=MODEL_WEIGHTS, save_best_only=True, save_weights_only=True, verbose=1)\n",
    "\n",
    "hparams_nrms.history_size = HISTORY_SIZE\n",
    "model = NRMSModel(\n",
    "    hparams=hparams_nrms,\n",
    "    word2vec_embedding=word2vec_embedding,\n",
    "    seed=42,\n",
    ")\n",
    "hist = model.model.fit(\n",
    "    train_dataloader,\n",
    "    validation_data=val_dataloader,\n",
    "    epochs=1,\n",
    "    callbacks=[tensorboard_callback, early_stopping_callback, modelcheckpoint_callback],\n",
    ")\n",
    "model.model.load_weights(filepath=MODEL_WEIGHTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 575ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<MetricEvaluator class>: \n",
       " {\n",
       "    \"auc\": 0.5068132911024271,\n",
       "    \"mrr\": 0.301223308572205,\n",
       "    \"ndcg@5\": 0.3368249540891147,\n",
       "    \"ndcg@10\": 0.43714083395568126\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ebrec.evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "\n",
    "pred_validation = model.scorer.predict(val_dataloader)\n",
    "\n",
    "df_validation = add_prediction_scores(df_validation, pred_validation.tolist()) \\\n",
    "    .pipe(add_known_user_column, known_users=df_train[DEFAULT_USER_COL])\n",
    "\n",
    "metrics = MetricEvaluator(\n",
    "    labels=df_validation[\"labels\"].to_list(),\n",
    "    predictions=df_validation[\"scores\"].to_list(),\n",
    "    metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)],\n",
    ")\n",
    "metrics.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 2s 665ms/step\n",
      "[[0.49938118]\n",
      " [0.4980004 ]\n",
      " [0.49865976]\n",
      " ...\n",
      " [0.50148994]\n",
      " [0.4993194 ]\n",
      " [0.49926835]]\n"
     ]
    }
   ],
   "source": [
    "# pred_test = model.scorer.predict(test_dataloader) <--- breaks because of size mismatch\n",
    "pred_test = model.scorer.predict(val_dataloader)\n",
    "print(pred_test)\n",
    "\n",
    "# store\n",
    "submission_path = Path(os.getcwd()).parent / \"submissions\" / \"ebnerd_nrms.txt\"\n",
    "with open(submission_path, \"w\") as f:\n",
    "    for idx, row in enumerate(pred_test):\n",
    "        f.write(f\"{idx} {row}\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
