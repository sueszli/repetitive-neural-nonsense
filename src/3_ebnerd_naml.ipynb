{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see paper: https://arxiv.org/pdf/1907.05576\n",
    "\n",
    "# NAML: Neural News Recommendation with Attentive Multi-View Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sueszli/.asdf/installs/python/3.11.9/lib/python3.11/site-packages/ebrec/utils/_behaviors.py:619: UserWarning: truncate_history: The history IDs expeced in ascending order\n",
      "  warnings.warn(f\"{function_name}: The history IDs expeced in ascending order\")\n",
      "/Users/sueszli/.asdf/installs/python/3.11.9/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/sueszli/.asdf/installs/python/3.11.9/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from ebrec.utils._constants import *\n",
    "from ebrec.utils._behaviors import create_binary_labels_column, sampling_strategy_wu2019, add_known_user_column, add_prediction_scores, truncate_history\n",
    "from ebrec.utils._articles import convert_text2encoding_with_transformers, create_article_id_to_value_mapping\n",
    "from ebrec.utils._polars import concat_str_columns, slice_join_dataframes\n",
    "from ebrec.utils._nlp import get_transformers_word_embeddings\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "load data\n",
    "\"\"\"\n",
    "\n",
    "data_base = Path(os.getcwd()).parent / \"data-merged\" / \"merged\"\n",
    "# train_val_base = data_base / \"1-ebnerd_demo_(20MB)\"\n",
    "train_val_base = data_base / \"2-ebnerd_small_(80MB)\"\n",
    "# train_val_base = data_base / \"3-ebnerd_large_(3.0GB)\"\n",
    "test_base = data_base / \"5-ebnerd_testset_(1.5GB)\"\n",
    "assert train_val_base.exists() and test_base.exists()\n",
    "\n",
    "train_behaviors = pl.scan_parquet(train_val_base / \"train\" / \"behaviors.parquet\")\n",
    "train_history = pl.scan_parquet(train_val_base / \"train\" / \"history.parquet\")\n",
    "\n",
    "val_behavior = pl.scan_parquet(train_val_base / \"validation\" / \"behaviors.parquet\")\n",
    "val_history = pl.scan_parquet(train_val_base / \"validation\" / \"history.parquet\")\n",
    "\n",
    "test_behavior = pl.scan_parquet(test_base / \"test\" / \"behaviors.parquet\")\n",
    "test_history = pl.scan_parquet(test_base / \"test\" / \"history.parquet\")\n",
    "\n",
    "train_articles: pl.LazyFrame = pl.scan_parquet(train_val_base / \"articles.parquet\")\n",
    "val_articles: pl.LazyFrame = train_articles\n",
    "test_articles: pl.LazyFrame = pl.scan_parquet(test_base / \"articles.parquet\")\n",
    "\n",
    "articles_word2vec: pl.LazyFrame = pl.scan_parquet(data_base / \"7-Ekstra-Bladet-word2vec_(133MB)\" / \"document_vector.parquet\")\n",
    "articles_image_embeddings: pl.LazyFrame = pl.scan_parquet(data_base / \"8-Ekstra_Bladet_image_embeddings_(372MB)\" / \"image_embeddings.parquet\")\n",
    "articles_contrastive_vector: pl.LazyFrame = pl.scan_parquet(data_base / \"9-Ekstra-Bladet-contrastive_vector_(341MB)\" / \"contrastive_vector.parquet\")\n",
    "articles_bert_base_multilingual_cased: pl.LazyFrame = pl.scan_parquet(data_base / \"10-google-bert-base-multilingual-cased_(344MB)\" / \"bert_base_multilingual_cased.parquet\")\n",
    "articles_xlm_roberta_base: pl.LazyFrame = pl.scan_parquet(data_base / \"11-FacebookAI-xlm-roberta-base_(341MB)\" / \"xlm_roberta_base.parquet\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "preprocessing: truncate user history, select subset of columns, join behavior and history, sample based on Wu2019, add binary labels column\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def ebnerd_from_path(history: pl.LazyFrame, behaviors: pl.LazyFrame, history_size: int = 30) -> pl.DataFrame:\n",
    "    df_history = history.select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL).pipe(truncate_history, column=DEFAULT_HISTORY_ARTICLE_ID_COL, history_size=history_size, padding_value=0)\n",
    "    df_behaviors = behaviors.collect().pipe(slice_join_dataframes, df2=df_history.collect(), on=DEFAULT_USER_COL, how=\"left\")\n",
    "    return df_behaviors\n",
    "\n",
    "\n",
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "]\n",
    "HISTORY_SIZE = 30\n",
    "N_SAMPLES = 100\n",
    "df_train = (\n",
    "    ebnerd_from_path(history=train_history, behaviors=train_behaviors, history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(sampling_strategy_wu2019, npratio=4, shuffle=True, with_replacement=True, seed=123)\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(n=N_SAMPLES)\n",
    ")\n",
    "df_validation = ebnerd_from_path(history=val_history, behaviors=val_behavior, history_size=HISTORY_SIZE).select(COLUMNS).pipe(create_binary_labels_column).sample(n=N_SAMPLES)\n",
    "df_test = (\n",
    "    ebnerd_from_path(history=test_history, behaviors=val_behavior, history_size=HISTORY_SIZE)\n",
    "    .with_columns(pl.Series(DEFAULT_CLICKED_ARTICLES_COL, [[]]))\n",
    "    .select(COLUMNS)\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(n=N_SAMPLES)\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "use huggingface transformers to convert article text to tokens, tokens to embeddings\n",
    "\"\"\"\n",
    "\n",
    "df_articles = train_articles.collect()\n",
    "TRANSFORMER_MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "TEXT_COLUMNS_TO_USE = [DEFAULT_SUBTITLE_COL, DEFAULT_TITLE_COL]\n",
    "MAX_TITLE_LENGTH = 30\n",
    "\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH)\n",
    "article_mapping = create_article_id_to_value_mapping(df=df_articles, value_col=token_col_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "batch data\n",
    "\"\"\"\n",
    "from ebrec.models.newsrec.dataloader import NAMLDataLoader\n",
    "\n",
    "DEFAULT_ARTICLE_MODIFIED_TIMESTAMP_COL = \"last_modified_time\"\n",
    "DEFAULT_ARTICLE_PUBLISHED_TIMESTAMP_COL = \"published_time\"\n",
    "DEFAULT_SENTIMENT_LABEL_COL = \"sentiment_label\"\n",
    "DEFAULT_SENTIMENT_SCORE_COL = \"sentiment_score\"\n",
    "DEFAULT_TOTAL_READ_TIME_COL = \"total_read_time\"\n",
    "DEFAULT_TOTAL_PAGEVIEWS_COL = \"total_pageviews\"\n",
    "DEFAULT_TOTAL_INVIEWS_COL = \"total_inviews\"\n",
    "DEFAULT_ARTICLE_TYPE_COL = \"article_type\"\n",
    "DEFAULT_CATEGORY_STR_COL = \"category_str\"\n",
    "DEFAULT_SUBCATEGORY_COL = \"subcategory\"\n",
    "DEFAULT_ENTITIES_COL = \"entity_groups\"\n",
    "DEFAULT_IMAGE_IDS_COL = \"image_ids\"\n",
    "DEFAULT_SUBTITLE_COL = \"subtitle\"\n",
    "DEFAULT_CATEGORY_COL = \"category\"\n",
    "DEFAULT_NER_COL = \"ner_clusters\"\n",
    "DEFAULT_PREMIUM_COL = \"premium\"\n",
    "DEFAULT_TOPICS_COL = \"topics\"\n",
    "DEFAULT_TITLE_COL = \"title\"\n",
    "DEFAULT_BODY_COL = \"body\"\n",
    "DEFAULT_URL_COL = \"url\"\n",
    "\n",
    "\n",
    "user_id_mapping: dict[int, int] = {user_id: i for i, user_id in enumerate(df_train[DEFAULT_USER_COL].unique())}\n",
    "body_mapping: dict[int, list[int]] = {article_id: embeddings for article_id, embeddings in zip(df_articles[DEFAULT_ARTICLE_ID_COL], df_articles[token_col_title])}\n",
    "category_mapping: dict[int, int] = {article_id: category for article_id, category in zip(df_articles[DEFAULT_ARTICLE_ID_COL], df_articles[DEFAULT_CATEGORY_COL])}\n",
    "subcategory_mapping: dict[int, int] = {article_id: subcategory for article_id, subcategory in zip(df_articles[DEFAULT_ARTICLE_ID_COL], df_articles[DEFAULT_SUBCATEGORY_COL])}\n",
    "\n",
    "\n",
    "train_dataloader = NAMLDataLoader(\n",
    "    body_mapping=body_mapping,\n",
    "    category_mapping=category_mapping,\n",
    "    subcategory_mapping=subcategory_mapping,\n",
    "\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    batch_size=64,\n",
    ")\n",
    "val_dataloader = NAMLDataLoader(\n",
    "    body_mapping=body_mapping,\n",
    "    category_mapping=category_mapping,\n",
    "    subcategory_mapping=subcategory_mapping,\n",
    "\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    batch_size=32,\n",
    ")\n",
    "test_dataloader = NAMLDataLoader(\n",
    "    body_mapping=body_mapping,\n",
    "    category_mapping=category_mapping,\n",
    "    subcategory_mapping=subcategory_mapping,\n",
    "\n",
    "    behaviors=df_test,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-11 21:26:59.290054: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2024-05-11 21:26:59.290102: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-05-11 21:26:59.290115: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-05-11 21:26:59.290165: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-05-11 21:26:59.290180: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (64, 30) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m NPAModel(hparams\u001b[38;5;241m=\u001b[39mconfig, word2vec_embedding\u001b[38;5;241m=\u001b[39mword2vec_embedding, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# model.model.summary()\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# epochs=1,\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelcheckpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_weights(filepath\u001b[38;5;241m=\u001b[39mMODEL_WEIGHTS)\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.11.9/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.11.9/lib/python3.11/site-packages/ebrec/models/newsrec/dataloader.py:319\u001b[0m, in \u001b[0;36mNAMLDataLoader.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    313\u001b[0m his_input_body \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m    314\u001b[0m     batch_X[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbody_prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory_column]\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[1;32m    315\u001b[0m )\n\u001b[1;32m    316\u001b[0m his_input_vert \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m    317\u001b[0m     batch_X[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategory_prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory_column]\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[1;32m    318\u001b[0m )[:, :, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[0;32m--> 319\u001b[0m his_input_subvert \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubcategory_prefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory_column\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[:, :, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# =>\u001b[39;00m\n\u001b[1;32m    323\u001b[0m pred_input_title \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m    324\u001b[0m     batch_X[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtitle_prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minview_col]\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[1;32m    325\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (64, 30) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "train model\n",
    "\"\"\"\n",
    "from ebrec.models.newsrec.model_config import hparams_npa\n",
    "from ebrec.models.newsrec.npa import NPAModel\n",
    "\n",
    "MODEL_NAME = \"NPA\"\n",
    "LOG_DIR = f\"./runs/{MODEL_NAME}\"\n",
    "MODEL_WEIGHTS = f\"./runs/data/state_dict/{MODEL_NAME}/weights\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=1)\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2)\n",
    "modelcheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=MODEL_WEIGHTS, save_best_only=True, save_weights_only=True, verbose=1)\n",
    "\n",
    "config = hparams_npa\n",
    "model = NPAModel(hparams=config, word2vec_embedding=word2vec_embedding, seed=42)\n",
    "# model.model.summary()\n",
    "hist = model.model.fit(\n",
    "    train_dataloader,\n",
    "    validation_data=val_dataloader,\n",
    "    # epochs=1,\n",
    "    callbacks=[tensorboard_callback, early_stopping_callback, modelcheckpoint_callback],\n",
    ")\n",
    "model.model.load_weights(filepath=MODEL_WEIGHTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 2s 365ms/step\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "evaluate performance\n",
    "\"\"\"\n",
    "\n",
    "pred_validation = model.scorer.predict(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>labels</th><th>scores</th><th>is_known_user</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i32]</td><td>list[i32]</td><td>list[i8]</td><td>list[f64]</td><td>bool</td></tr></thead><tbody><tr><td>2094919</td><td>[9773351, 9774187, … 9779860]</td><td>[9780702, 9788188, … 9787499]</td><td>[9787499]</td><td>[0, 0, … 1]</td><td>[0.49845, 0.497381, … 0.497841]</td><td>false</td></tr><tr><td>2029201</td><td>[9778326, 9777941, … 9779738]</td><td>[9783405, 9783852, … 8496358]</td><td>[9783852]</td><td>[0, 1, … 0]</td><td>[0.49801, 0.497147, … 0.498185]</td><td>false</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 7)\n",
       "┌─────────┬──────────────┬──────────────┬──────────────┬─────────────┬──────────────┬──────────────┐\n",
       "│ user_id ┆ article_id_f ┆ article_ids_ ┆ article_ids_ ┆ labels      ┆ scores       ┆ is_known_use │\n",
       "│ ---     ┆ ixed         ┆ inview       ┆ clicked      ┆ ---         ┆ ---          ┆ r            │\n",
       "│ u32     ┆ ---          ┆ ---          ┆ ---          ┆ list[i8]    ┆ list[f64]    ┆ ---          │\n",
       "│         ┆ list[i32]    ┆ list[i32]    ┆ list[i32]    ┆             ┆              ┆ bool         │\n",
       "╞═════════╪══════════════╪══════════════╪══════════════╪═════════════╪══════════════╪══════════════╡\n",
       "│ 2094919 ┆ [9773351,    ┆ [9780702,    ┆ [9787499]    ┆ [0, 0, … 1] ┆ [0.49845,    ┆ false        │\n",
       "│         ┆ 9774187, …   ┆ 9788188, …   ┆              ┆             ┆ 0.497381, …  ┆              │\n",
       "│         ┆ 9779860]     ┆ 9787499]     ┆              ┆             ┆ 0.497841]    ┆              │\n",
       "│ 2029201 ┆ [9778326,    ┆ [9783405,    ┆ [9783852]    ┆ [0, 1, … 0] ┆ [0.49801,    ┆ false        │\n",
       "│         ┆ 9777941, …   ┆ 9783852, …   ┆              ┆             ┆ 0.497147, …  ┆              │\n",
       "│         ┆ 9779738]     ┆ 8496358]     ┆              ┆             ┆ 0.498185]    ┆              │\n",
       "└─────────┴──────────────┴──────────────┴──────────────┴─────────────┴──────────────┴──────────────┘"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation = add_prediction_scores(df_validation, pred_validation.tolist()).pipe(\n",
    "    add_known_user_column, known_users=df_train[DEFAULT_USER_COL]\n",
    ")\n",
    "df_validation.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MetricEvaluator class>: \n",
       " {\n",
       "    \"auc\": 0.4832519055489523,\n",
       "    \"mrr\": 0.30877717899621926,\n",
       "    \"ndcg@5\": 0.32656298038911985,\n",
       "    \"ndcg@10\": 0.4381692958258077\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ebrec.evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "\n",
    "metrics = MetricEvaluator(\n",
    "    labels=df_validation[\"labels\"].to_list(),\n",
    "    predictions=df_validation[\"scores\"].to_list(),\n",
    "    metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)],\n",
    ")\n",
    "metrics.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 329ms/step\n"
     ]
    }
   ],
   "source": [
    "# pred_test = model.scorer.predict(test_dataloader) <--- breaks because of size mismatch\n",
    "pred_test = model.scorer.predict(val_dataloader)\n",
    "\n",
    "# store\n",
    "submission_path = Path(os.getcwd()).parent / \"submissions\" / \"ebnerd_lstur.txt\"\n",
    "with open(submission_path, \"w\") as f:\n",
    "    for idx, row in enumerate(pred_test):\n",
    "        f.write(f\"{idx} {row}\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
